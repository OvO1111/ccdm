dataset:
  train:
    target: datasets.ruijin.CLIPDataset
    params:
      split: train
      max_size: null
      force_collate_len: 16
  validation:
    target: datasets.ruijin.CLIPDataset
    params:
      split: val
      max_size: 10
      force_collate_len: 16

model:
  target: train.clip_trainer.CLIPModel
  params:
    embed_dim: 256
    image_encoder_spec:
      target: ddpm.models.unet_openai.clip_encoder.UNetEncoderHalf
      params:
        in_channels: 1
        model_channels: 64
        num_res_blocks: 2
        cond_encoded_shape: null
        num_heads: 1
        num_head_channels: 32
        channel_mult: [1, 2, 2, 4, 8]
        attention_resolutions: [32, 16, 8]
        use_checkpoint: true
        dims: 3
        use_spatial_transformer: true
        transformer_depth: 1
        context_dim: 5120
        ce_head: false
    text_encoder_spec:
      target: ddpm.models.unet_openai.clip_encoder.TextMapper
      params:
        out_channels: 1
        kernel_size: 1

trainer:
  target: train.clip_trainer.Trainer
  params:
    batch_size: 4
    max_epochs: 100
    timesteps: 1000
    lr: 1e-4
    snapshot_path: /mnt/workspace/dailinrui/data/pretrained/ccdm/clip/test
    parallel_validation: false